{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:51:31.378154Z",
     "start_time": "2025-03-25T14:51:31.346526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "4eb4ebb63c9770bd",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-25T15:09:59.985595Z",
     "start_time": "2025-03-25T15:09:59.639045Z"
    }
   },
   "source": [
    "from src import DocumentIndexer\n",
    "indexer = DocumentIndexer()\n",
    "indexer.load_retriever()\n",
    "\n",
    "query = \"What is prompt engineering?\"\n",
    "results = indexer.fusion_retrieval(query)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Chroma from ./../storage\n",
      "Loading BM25 from ./../storage/bm25.pkl\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T15:23:12.223555Z",
     "start_time": "2025-03-25T15:23:11.627979Z"
    }
   },
   "cell_type": "code",
   "source": "type(results[0])",
   "id": "ab2f4b619e0e65e2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T15:10:29.671186Z",
     "start_time": "2025-03-25T15:10:29.599203Z"
    }
   },
   "cell_type": "code",
   "source": "print(results[0].page_content)",
   "id": "ebe10cf360d9528b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: What type of water formation is formed by clouds?\n",
      "Knowledge: Clouds are made of water vapor.\n",
      "\n",
      "Input: {question}\n",
      "Knowledge:\n",
      "And then with model-generated knowledge, prompt the LM further to get the answer.\n",
      "Programming Language#\n",
      "Both PAL (Program-aided language models); Gao et al. 2022) and PoT (Program of Thoughts prompting; Chen et al. 2022) ask LLM to generate programming language statements to resolve natural language reasoning problems, hence offloading the solution step to a runtime such as a Python interpreter. Such setup decouples complex computation and reasoning. It relies on a LM with good enough coding skills.\n",
      "\n",
      "Fig. 3. Comparing CoT and PoT. (Image source: Chen et al. 2022).\n",
      "External APIs#\n",
      "TALM (Tool Augmented Language Models; Parisi et al. 2022) is a language model augmented with text-to-text API calls. LM is guided to generate |tool-call and tool input text conditioned on task input text to construct API call requests. When |result shows up, the specified tool API is called and the returned result gets appended to the text sequence. The final output is generated following |output token.\n",
      "\n",
      "Fig. 4. The format of API calls in TALM. (Image source: Parisi et al. 2022).\n",
      "TALM adopts a self-play approach to iteratively bootstrap the dataset of tool use examples and finetune LM with it. This self-play, defined as a model interacting with a tool API, iteratively expands the dataset based on whether a newly added tool API can improve the model outputs. Same idea is adopted in Toolformer too, described in more details below. The pipeline loosely mimics a RL process where LM is the policy network and it is trained by policy gradient with a binary reward signal.\n",
      "\n",
      "Fig. 5. Self-play iterations help boost the model performance.(Image source: Parisi et al. 2022).\n",
      "Toolformer (Schick et al. 2023) is a LM that can use external tools via simple APIs, which is built in a self-supervised manner and only requires a handful of demonstrations for each API. The toolbox of Toolformer includes:\n",
      "\n",
      "Calculator to help LM with the lack of precise math skills;\n",
      "Q&A system to help with unfaithful content and hallucination;\n",
      "Search engine to provide up-to-date information after pretraining cut off time;\n",
      "Translation system to improve performance on low resource language;\n",
      "Calendar to make LM be aware of time progression.\n",
      "\n",
      "\n",
      "Fig. 6. Illustration of how to build Toolformer.(Image source: Schick et al. 2023).\n",
      "Toolformer is trained as follows:\n",
      "\n",
      "\n",
      "Prompting to annotate potential API calls. Ask a pre-trained LM to annotate a dataset via few-shot learning with API call usage examples. Formatting example:\n",
      "\n",
      "Fig. 7. How dataset is annotated to do API calls.(Image source: Schick et al. 2023).\n",
      "\n",
      "\n",
      "Each API call is represented as a tuple of (API name, corresponding input), $c=(a_c, i_c)$ and its corresponding result is denoted as $r$. The API call sequences with and without results are labeled as follows, respectively:\n",
      "\n",
      "  $$\n",
      "  \\begin{aligned}\n",
      "  e(c) &= \\langle\\texttt{API}\\rangle a_c(i_c) \\langle\\texttt{/API}\\rangle \\\\\n",
      "  e(c, r) &= \\langle\\texttt{API}\\rangle a_c(i_c) \\to r \\langle\\texttt{/API}\\rangle\n",
      "  \\end{aligned}\n",
      "  $$\n",
      "  \n",
      "\n",
      "\n",
      "Sample API calls based on the probabilities $p_\\text{LM}(\\langle\\texttt{API}\\rangle \\mid \\text{prompt}(\\mathbf{x}), \\mathbf{x}_{1:i})$ and select top $k$ candidate positions for doing API calls at position $i$ if the probability is larger than a threshold.\n",
      "\n",
      "\n",
      "Then we sample potential API calls from the LM given the sequence $[\\text{prompt}(\\mathbf{x}), x_1, \\dots, x_{i-1}, \\langle\\texttt{API}\\rangle]$ as prefix and $\\langle\\texttt{/API}\\rangle$ as suffix.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Filter annotations based on whether API calls help model predict future tokens. Use a self-supervised loss to decide which API calls are actually helpful.\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
